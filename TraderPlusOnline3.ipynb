{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEUcInXXQQ0dNSW1tx6ivj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulliyevn/DeepLearningModels/blob/main/TraderPlusOnline3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uALrVukPF8L2",
        "outputId": "66ba98e8-c3de-4772-b86a-2bca9e3a04a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Save path ready: /content/drive/MyDrive/KERAS\n",
            "ğŸš€ BTC LSTM Trading Model v3.0 - Multi-Timeframe Ensemble\n",
            "============================================================\n",
            "ğŸ“Š Loading multi-timeframe data...\n",
            "ğŸ“¡ API Status: {'timestamp': '2025-06-20 21:00:41', 'current_usage': 1, 'plan_limit': 55, 'plan_category': 'grow'}\n",
            "\n",
            "ğŸ”„ Loading 5min...\n",
            "ğŸ”„ Trying symbol: BTCUSD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['code', 'message', 'status']\n",
            "ğŸ“ API Message: **symbol** or **figi** parameter is missing or invalid. Please provide a valid symbol according to API documentation: https://twelvedata.com/docs#reference-data\n",
            "ğŸ”„ Trying symbol: BTC/USD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['meta', 'values', 'status']\n",
            "âœ… Raw data shape: (1000, 5)\n",
            "ğŸ“‹ Columns: ['datetime', 'open', 'high', 'low', 'close']\n",
            "ğŸ“Š Created synthetic volume column\n",
            "âœ… 5min data loaded: 1000 records with symbol BTC/USD\n",
            "\n",
            "ğŸ”„ Loading 15min...\n",
            "ğŸ”„ Trying symbol: BTCUSD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['code', 'message', 'status']\n",
            "ğŸ“ API Message: **symbol** or **figi** parameter is missing or invalid. Please provide a valid symbol according to API documentation: https://twelvedata.com/docs#reference-data\n",
            "ğŸ”„ Trying symbol: BTC/USD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['meta', 'values', 'status']\n",
            "âœ… Raw data shape: (1000, 5)\n",
            "ğŸ“‹ Columns: ['datetime', 'open', 'high', 'low', 'close']\n",
            "ğŸ“Š Created synthetic volume column\n",
            "âœ… 15min data loaded: 1000 records with symbol BTC/USD\n",
            "\n",
            "ğŸ”„ Loading 1h...\n",
            "ğŸ”„ Trying symbol: BTCUSD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['code', 'message', 'status']\n",
            "ğŸ“ API Message: **symbol** or **figi** parameter is missing or invalid. Please provide a valid symbol according to API documentation: https://twelvedata.com/docs#reference-data\n",
            "ğŸ”„ Trying symbol: BTC/USD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['meta', 'values', 'status']\n",
            "âœ… Raw data shape: (1000, 5)\n",
            "ğŸ“‹ Columns: ['datetime', 'open', 'high', 'low', 'close']\n",
            "ğŸ“Š Created synthetic volume column\n",
            "âœ… 1h data loaded: 1000 records with symbol BTC/USD\n",
            "\n",
            "âœ… Loaded 3 timeframes\n",
            "ğŸ”§ Creating multi-timeframe features...\n",
            "Processing 5min...\n",
            "Processing 15min...\n",
            "Processing 1h...\n",
            "âœ… Multi-timeframe features created: (1000, 50)\n",
            "ğŸ“Š Final dataset shape: (1000, 50)\n",
            "Features: ['datetime', 'open', 'high', 'low', 'close', 'volume', 'returns', 'log_returns', 'price_range', 'body_size', 'upper_shadow', 'lower_shadow', 'volume_sma', 'volume_ratio', 'price_volume', 'vwap', 'volatility', 'atr', 'rsi', 'macd', 'macd_signal', 'bb_upper', 'bb_lower', 'sma_5', 'ema_5', 'price_sma_5_ratio', 'sma_10', 'ema_10', 'price_sma_10_ratio', 'sma_20', 'ema_20', 'price_sma_20_ratio', 'sma_50', 'ema_50', 'price_sma_50_ratio', 'market_structure', 'liquidity_sweep', 'fvg_signal', 'order_block', 'displacement', 'h1_market_structure', 'h1_rsi', 'h1_macd', 'h1_volatility', 'h1_price_sma_20_ratio', 'm5_liquidity_sweep', 'm5_fvg_signal', 'm5_displacement', 'm5_volatility', 'm5_volume_ratio']\n",
            "ğŸ“¥ Loading base model v2.0...\n",
            "âœ… Base model v2.0 loaded successfully\n",
            "ğŸš€ Training ensemble model...\n",
            "ğŸ¯ Creating trading signals (threshold: 0.4%)...\n",
            "Signal distribution: {0: np.int64(511), 2: np.int64(253), 1: np.int64(236)}\n",
            "ğŸ“Š Preparing sequences...\n",
            "âœ… Sequences prepared: (936, 64, 52), targets: (936,)\n",
            "Training data: (748, 64, 52), Validation: (188, 64, 52)\n",
            "ğŸ§  Creating enhanced LSTM architecture...\n",
            "ğŸ“ Training enhanced LSTM...\n",
            "Epoch 1/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 167ms/step - accuracy: 0.3221 - loss: 1.7378 - val_accuracy: 0.5745 - val_loss: 1.0112 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.3924 - loss: 1.4527 - val_accuracy: 0.6702 - val_loss: 0.9238 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 112ms/step - accuracy: 0.3730 - loss: 1.4025 - val_accuracy: 0.6702 - val_loss: 0.8973 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 101ms/step - accuracy: 0.4452 - loss: 1.2167 - val_accuracy: 0.6809 - val_loss: 0.8856 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 135ms/step - accuracy: 0.4364 - loss: 1.1758 - val_accuracy: 0.6436 - val_loss: 0.9096 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.4904 - loss: 1.1147 - val_accuracy: 0.5691 - val_loss: 0.9008 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 114ms/step - accuracy: 0.4843 - loss: 1.0819 - val_accuracy: 0.5053 - val_loss: 0.9387 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 108ms/step - accuracy: 0.5181 - loss: 1.0180 - val_accuracy: 0.5532 - val_loss: 0.9276 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 136ms/step - accuracy: 0.5597 - loss: 0.9542 - val_accuracy: 0.6064 - val_loss: 0.9244 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - accuracy: 0.5572 - loss: 0.9899 - val_accuracy: 0.6117 - val_loss: 0.9140 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 115ms/step - accuracy: 0.5099 - loss: 1.0586 - val_accuracy: 0.5691 - val_loss: 0.9166 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 113ms/step - accuracy: 0.5820 - loss: 0.8944 - val_accuracy: 0.5319 - val_loss: 0.9197 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step - accuracy: 0.5705 - loss: 0.9555 - val_accuracy: 0.5798 - val_loss: 0.9072 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - accuracy: 0.5775 - loss: 0.9127 - val_accuracy: 0.6277 - val_loss: 0.8728 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 126ms/step - accuracy: 0.6075 - loss: 0.8927 - val_accuracy: 0.6596 - val_loss: 0.8278 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 119ms/step - accuracy: 0.5656 - loss: 0.9060 - val_accuracy: 0.6383 - val_loss: 0.8277 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.6010 - loss: 0.8678 - val_accuracy: 0.6383 - val_loss: 0.8074 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.6408 - loss: 0.8240 - val_accuracy: 0.6649 - val_loss: 0.8016 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 130ms/step - accuracy: 0.6249 - loss: 0.8475 - val_accuracy: 0.6383 - val_loss: 0.8312 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 110ms/step - accuracy: 0.5792 - loss: 0.8520 - val_accuracy: 0.6649 - val_loss: 0.7909 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 131ms/step - accuracy: 0.5956 - loss: 0.8986 - val_accuracy: 0.7553 - val_loss: 0.7240 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 108ms/step - accuracy: 0.6661 - loss: 0.7839 - val_accuracy: 0.7660 - val_loss: 0.7100 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.6444 - loss: 0.8062 - val_accuracy: 0.7660 - val_loss: 0.7082 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 105ms/step - accuracy: 0.6484 - loss: 0.7887 - val_accuracy: 0.7553 - val_loss: 0.7080 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.6902 - loss: 0.7624 - val_accuracy: 0.7553 - val_loss: 0.7069 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 103ms/step - accuracy: 0.7143 - loss: 0.6951 - val_accuracy: 0.7553 - val_loss: 0.7081 - learning_rate: 0.0010\n",
            "Epoch 27/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 131ms/step - accuracy: 0.6730 - loss: 0.7829 - val_accuracy: 0.7606 - val_loss: 0.6986 - learning_rate: 0.0010\n",
            "Epoch 28/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 104ms/step - accuracy: 0.6643 - loss: 0.7966 - val_accuracy: 0.7660 - val_loss: 0.6677 - learning_rate: 0.0010\n",
            "Epoch 29/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 125ms/step - accuracy: 0.6696 - loss: 0.7444 - val_accuracy: 0.7553 - val_loss: 0.6766 - learning_rate: 0.0010\n",
            "Epoch 30/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - accuracy: 0.7282 - loss: 0.7039 - val_accuracy: 0.7553 - val_loss: 0.6819 - learning_rate: 0.0010\n",
            "Epoch 31/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 126ms/step - accuracy: 0.7154 - loss: 0.7313 - val_accuracy: 0.7394 - val_loss: 0.7046 - learning_rate: 0.0010\n",
            "Epoch 32/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 104ms/step - accuracy: 0.6643 - loss: 0.8099 - val_accuracy: 0.7021 - val_loss: 0.7258 - learning_rate: 0.0010\n",
            "Epoch 33/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 131ms/step - accuracy: 0.7086 - loss: 0.7044 - val_accuracy: 0.7340 - val_loss: 0.7184 - learning_rate: 0.0010\n",
            "Epoch 34/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.7081 - loss: 0.7173 - val_accuracy: 0.6809 - val_loss: 0.7573 - learning_rate: 0.0010\n",
            "Epoch 35/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 110ms/step - accuracy: 0.6596 - loss: 0.7336 - val_accuracy: 0.7234 - val_loss: 0.7025 - learning_rate: 0.0010\n",
            "Epoch 36/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 99ms/step - accuracy: 0.7048 - loss: 0.7279 - val_accuracy: 0.7340 - val_loss: 0.6893 - learning_rate: 0.0010\n",
            "Epoch 37/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 138ms/step - accuracy: 0.7138 - loss: 0.6869 - val_accuracy: 0.7287 - val_loss: 0.6687 - learning_rate: 0.0010\n",
            "Epoch 38/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 99ms/step - accuracy: 0.7008 - loss: 0.7002 - val_accuracy: 0.7500 - val_loss: 0.6699 - learning_rate: 0.0010\n",
            "Epoch 39/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 131ms/step - accuracy: 0.7132 - loss: 0.6730 - val_accuracy: 0.7606 - val_loss: 0.6669 - learning_rate: 5.0000e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - accuracy: 0.7195 - loss: 0.7111 - val_accuracy: 0.7447 - val_loss: 0.6818 - learning_rate: 5.0000e-04\n",
            "Epoch 41/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 128ms/step - accuracy: 0.7159 - loss: 0.7046 - val_accuracy: 0.7500 - val_loss: 0.6909 - learning_rate: 5.0000e-04\n",
            "Epoch 42/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 112ms/step - accuracy: 0.7056 - loss: 0.6812 - val_accuracy: 0.7447 - val_loss: 0.6938 - learning_rate: 5.0000e-04\n",
            "Epoch 43/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 158ms/step - accuracy: 0.7604 - loss: 0.6332 - val_accuracy: 0.7553 - val_loss: 0.6901 - learning_rate: 5.0000e-04\n",
            "Epoch 44/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 103ms/step - accuracy: 0.7470 - loss: 0.6606 - val_accuracy: 0.7553 - val_loss: 0.6714 - learning_rate: 5.0000e-04\n",
            "Epoch 45/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 136ms/step - accuracy: 0.7358 - loss: 0.6993 - val_accuracy: 0.7447 - val_loss: 0.6631 - learning_rate: 5.0000e-04\n",
            "Epoch 46/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - accuracy: 0.7198 - loss: 0.6813 - val_accuracy: 0.7500 - val_loss: 0.6617 - learning_rate: 5.0000e-04\n",
            "Epoch 47/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.7525 - loss: 0.6390 - val_accuracy: 0.7606 - val_loss: 0.6792 - learning_rate: 5.0000e-04\n",
            "Epoch 48/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 133ms/step - accuracy: 0.7201 - loss: 0.6659 - val_accuracy: 0.7713 - val_loss: 0.6616 - learning_rate: 5.0000e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 118ms/step - accuracy: 0.7360 - loss: 0.6864 - val_accuracy: 0.7713 - val_loss: 0.6463 - learning_rate: 5.0000e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.7362 - loss: 0.6548 - val_accuracy: 0.7766 - val_loss: 0.6294 - learning_rate: 5.0000e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 131ms/step - accuracy: 0.7548 - loss: 0.6301 - val_accuracy: 0.7819 - val_loss: 0.6253 - learning_rate: 5.0000e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 107ms/step - accuracy: 0.7163 - loss: 0.6680 - val_accuracy: 0.7872 - val_loss: 0.6200 - learning_rate: 5.0000e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 136ms/step - accuracy: 0.7531 - loss: 0.6104 - val_accuracy: 0.7660 - val_loss: 0.6206 - learning_rate: 5.0000e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.7193 - loss: 0.6834 - val_accuracy: 0.7872 - val_loss: 0.6273 - learning_rate: 5.0000e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - accuracy: 0.7381 - loss: 0.6690 - val_accuracy: 0.7979 - val_loss: 0.6131 - learning_rate: 5.0000e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 101ms/step - accuracy: 0.7251 - loss: 0.6663 - val_accuracy: 0.7819 - val_loss: 0.6360 - learning_rate: 5.0000e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 138ms/step - accuracy: 0.7361 - loss: 0.6510 - val_accuracy: 0.7766 - val_loss: 0.6177 - learning_rate: 5.0000e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 103ms/step - accuracy: 0.7520 - loss: 0.6274 - val_accuracy: 0.7766 - val_loss: 0.6159 - learning_rate: 5.0000e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 130ms/step - accuracy: 0.7456 - loss: 0.6144 - val_accuracy: 0.7872 - val_loss: 0.6399 - learning_rate: 5.0000e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 116ms/step - accuracy: 0.7641 - loss: 0.6220 - val_accuracy: 0.7766 - val_loss: 0.6324 - learning_rate: 5.0000e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 108ms/step - accuracy: 0.7489 - loss: 0.6398 - val_accuracy: 0.7553 - val_loss: 0.6633 - learning_rate: 5.0000e-04\n",
            "Epoch 62/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 137ms/step - accuracy: 0.7432 - loss: 0.6349 - val_accuracy: 0.7713 - val_loss: 0.6582 - learning_rate: 5.0000e-04\n",
            "Epoch 63/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 104ms/step - accuracy: 0.7432 - loss: 0.6569 - val_accuracy: 0.7713 - val_loss: 0.6422 - learning_rate: 5.0000e-04\n",
            "Epoch 64/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 153ms/step - accuracy: 0.7140 - loss: 0.6749 - val_accuracy: 0.7872 - val_loss: 0.6332 - learning_rate: 5.0000e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 104ms/step - accuracy: 0.7887 - loss: 0.5357 - val_accuracy: 0.7819 - val_loss: 0.6426 - learning_rate: 5.0000e-04\n",
            "Epoch 66/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 132ms/step - accuracy: 0.7729 - loss: 0.6367 - val_accuracy: 0.7872 - val_loss: 0.6425 - learning_rate: 2.5000e-04\n",
            "Epoch 67/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - accuracy: 0.7701 - loss: 0.6030 - val_accuracy: 0.7819 - val_loss: 0.6407 - learning_rate: 2.5000e-04\n",
            "Epoch 68/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - accuracy: 0.7595 - loss: 0.5821 - val_accuracy: 0.7872 - val_loss: 0.6396 - learning_rate: 2.5000e-04\n",
            "Epoch 69/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 107ms/step - accuracy: 0.7271 - loss: 0.7000 - val_accuracy: 0.7926 - val_loss: 0.6380 - learning_rate: 2.5000e-04\n",
            "Epoch 70/100\n",
            "\u001b[1m47/47\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 132ms/step - accuracy: 0.7664 - loss: 0.6421 - val_accuracy: 0.7872 - val_loss: 0.6359 - learning_rate: 2.5000e-04\n",
            "\u001b[1m6/6\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 202ms/step\n",
            "âœ… LSTM Validation Accuracy: 0.7979\n",
            "ğŸŒ² Training XGBoost...\n",
            "âœ… XGBoost Accuracy: 0.4650\n",
            "ğŸ’¾ Saving model components...\n",
            "âœ… LSTM saved: /content/drive/MyDrive/KERAS/btc_lstm_v3_multitimeframe.keras\n",
            "âœ… XGBoost saved: /content/drive/MyDrive/KERAS/btc_ensemble_v3.pkl\n",
            "âœ… Scalers saved: /content/drive/MyDrive/KERAS/btc_scalers_v3.pkl\n",
            "ğŸ“ˆ Running memory-efficient backtest...\n",
            "Processing chunk 1/2\n",
            "ğŸ“Š Preparing sequences...\n",
            "âœ… Sequences prepared: (936, 64, 52), targets: (936,)\n",
            "ğŸ“Š Calculating performance metrics...\n",
            "\n",
            "============================================================\n",
            "ğŸ“ˆ BACKTEST RESULTS - BTC LSTM v3.0\n",
            "============================================================\n",
            "ğŸ’° Total Return: 0.53%\n",
            "ğŸ¯ Accuracy: 86.90%\n",
            "âœ… Win Rate: 100.00%\n",
            "ğŸ“Š Sharpe Ratio: 1.878\n",
            "ğŸ“‰ Max Drawdown: 0.00%\n",
            "ğŸ”„ Number of Trades: 31\n",
            "ğŸ’µ Final Capital: $10,052.60\n",
            "\n",
            "========================================\n",
            "ğŸ“Š COMPARISON WITH v2.0\n",
            "========================================\n",
            "v2.0 Results: 66% accuracy, +0.2% return, 0.21 Sharpe\n",
            "v3.0 Results: 86.9% accuracy, +0.5% return, 1.88 Sharpe\n",
            "ğŸš€ Improvement: +0.33% return\n",
            "ğŸ’¾ Results saved: /content/drive/MyDrive/KERAS/btc_v3_results.csv\n",
            "\n",
            "âœ… BTC LSTM v3.0 training and evaluation completed!\n"
          ]
        }
      ],
      "source": [
        "# BTC LSTM Trading Model v3.0 - Multi-Timeframe Ensemble\n",
        "# Enhanced version with fine-tuning, SMC patterns, and memory efficiency\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to install yfinance if not available\n",
        "try:\n",
        "    import yfinance as yf\n",
        "except ImportError:\n",
        "    print(\"ğŸ“¦ Installing yfinance...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"yfinance\"])\n",
        "    import yfinance as yf\n",
        "\n",
        "# TensorFlow/Keras imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "import gc\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Configuration\n",
        "SAVE_PATH = '/content/drive/MyDrive/KERAS'\n",
        "API_KEY = '2f88eb1e7f9b49ef884557f27c95bd37'\n",
        "BASE_URL = 'https://api.twelvedata.com'\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "print(f\"âœ… Save path ready: {SAVE_PATH}\")\n",
        "\n",
        "class BTCDataLoader:\n",
        "    \"\"\"Enhanced data loader for multiple timeframes with memory efficiency\"\"\"\n",
        "\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "        self.base_url = 'https://api.twelvedata.com'\n",
        "\n",
        "    def test_api_connection(self):\n",
        "        \"\"\"Test API connection and limits\"\"\"\n",
        "        url = f\"{self.base_url}/api_usage\"\n",
        "        params = {'apikey': self.api_key}\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, params=params, timeout=10)\n",
        "            usage_data = response.json()\n",
        "            print(f\"ğŸ“¡ API Status: {usage_data}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ API test failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def fetch_data(self, symbol='BTCUSD', interval='15min', outputsize=5000):\n",
        "        \"\"\"Fetch data with enhanced error handling and multiple symbol formats\"\"\"\n",
        "\n",
        "        # Try different symbol formats\n",
        "        symbols_to_try = [symbol, 'BTC/USD', 'BTCUSD', 'BTC-USD']\n",
        "\n",
        "        for sym in symbols_to_try:\n",
        "            print(f\"ğŸ”„ Trying symbol: {sym}\")\n",
        "\n",
        "            url = f\"{self.base_url}/time_series\"\n",
        "            params = {\n",
        "                'symbol': sym,\n",
        "                'interval': interval,\n",
        "                'apikey': self.api_key,\n",
        "                'outputsize': min(outputsize, 1000),  # Reduce initial request\n",
        "                'format': 'JSON'\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = requests.get(url, params=params, timeout=30)\n",
        "                print(f\"ğŸ“¡ Response status: {response.status_code}\")\n",
        "\n",
        "                data = response.json()\n",
        "                print(f\"ğŸ“„ Response keys: {list(data.keys())}\")\n",
        "\n",
        "                # Handle different response formats\n",
        "                if 'values' in data and data['values']:\n",
        "                    df = pd.DataFrame(data['values'])\n",
        "                    print(f\"âœ… Raw data shape: {df.shape}\")\n",
        "                    print(f\"ğŸ“‹ Columns: {df.columns.tolist()}\")\n",
        "\n",
        "                    # Check for required columns\n",
        "                    required_cols = ['datetime', 'open', 'high', 'low', 'close']\n",
        "                    missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "\n",
        "                    if missing_cols:\n",
        "                        print(f\"âš ï¸ Missing columns: {missing_cols}\")\n",
        "                        continue\n",
        "\n",
        "                    df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "                    df = df.sort_values('datetime').reset_index(drop=True)\n",
        "\n",
        "                    # Convert to numeric (handle volume separately)\n",
        "                    price_cols = ['open', 'high', 'low', 'close']\n",
        "                    for col in price_cols:\n",
        "                        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "                    # Handle volume (may not exist for crypto)\n",
        "                    if 'volume' in df.columns:\n",
        "                        df['volume'] = pd.to_numeric(df['volume'], errors='coerce')\n",
        "                    else:\n",
        "                        # Create synthetic volume based on price movement\n",
        "                        df['volume'] = abs(df['close'].pct_change()) * 1000000\n",
        "                        print(\"ğŸ“Š Created synthetic volume column\")\n",
        "\n",
        "                    # Remove rows with NaN prices\n",
        "                    initial_len = len(df)\n",
        "                    df = df.dropna(subset=price_cols)\n",
        "                    if len(df) < initial_len:\n",
        "                        print(f\"ğŸ§¹ Removed {initial_len - len(df)} rows with NaN prices\")\n",
        "\n",
        "                    if len(df) > 100:  # Minimum viable dataset\n",
        "                        print(f\"âœ… {interval} data loaded: {len(df)} records with symbol {sym}\")\n",
        "                        return df\n",
        "                    else:\n",
        "                        print(f\"âš ï¸ Too few records: {len(df)}\")\n",
        "                        continue\n",
        "\n",
        "                elif 'message' in data:\n",
        "                    print(f\"ğŸ“ API Message: {data['message']}\")\n",
        "                    if 'limit' in data['message'].lower():\n",
        "                        print(\"â³ API limit reached, waiting...\")\n",
        "                        time.sleep(10)\n",
        "                        continue\n",
        "                elif 'status' in data and data['status'] == 'error':\n",
        "                    print(f\"âŒ API Error: {data}\")\n",
        "                    continue\n",
        "                else:\n",
        "                    print(f\"â“ Unexpected response format: {data}\")\n",
        "                    continue\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                print(f\"â° Timeout for symbol {sym}\")\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error with symbol {sym}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"âŒ All symbol formats failed for {interval}\")\n",
        "        return None\n",
        "\n",
        "    def fetch_fallback_data(self, interval='15min'):\n",
        "        \"\"\"Fallback to yfinance if available\"\"\"\n",
        "        try:\n",
        "            import yfinance as yf\n",
        "            print(f\"ğŸ”„ Trying yfinance fallback for {interval}...\")\n",
        "\n",
        "            # Convert interval format\n",
        "            yf_interval_map = {\n",
        "                '5min': '5m',\n",
        "                '15min': '15m',\n",
        "                '1h': '1h'\n",
        "            }\n",
        "\n",
        "            if interval not in yf_interval_map:\n",
        "                return None\n",
        "\n",
        "            yf_interval = yf_interval_map[interval]\n",
        "\n",
        "            # Download BTC data\n",
        "            ticker = yf.Ticker(\"BTC-USD\")\n",
        "            df = ticker.history(period=\"1y\", interval=yf_interval)\n",
        "\n",
        "            if df.empty:\n",
        "                return None\n",
        "\n",
        "            # Convert to our format\n",
        "            df = df.reset_index()\n",
        "            df.columns = [col.lower() for col in df.columns]\n",
        "\n",
        "            # Rename columns to match our format\n",
        "            if 'date' in df.columns:\n",
        "                df = df.rename(columns={'date': 'datetime'})\n",
        "            elif 'datetime' not in df.columns:\n",
        "                df['datetime'] = df.index\n",
        "\n",
        "            # Ensure volume exists\n",
        "            if 'volume' not in df.columns:\n",
        "                df['volume'] = abs(df['close'].pct_change()) * 1000000\n",
        "\n",
        "            print(f\"âœ… Fallback {interval} data loaded: {len(df)} records\")\n",
        "            return df\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"âš ï¸ yfinance not available\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Fallback failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def generate_synthetic_data(self, interval='15min', periods=2000):\n",
        "        \"\"\"Generate synthetic BTC data for testing\"\"\"\n",
        "        print(f\"ğŸ² Generating synthetic {interval} data...\")\n",
        "\n",
        "        # Create date range\n",
        "        if interval == '5min':\n",
        "            freq = '5min'\n",
        "        elif interval == '15min':\n",
        "            freq = '15min'\n",
        "        else:\n",
        "            freq = '1h'\n",
        "\n",
        "        dates = pd.date_range(end=datetime.now(), periods=periods, freq=freq)\n",
        "\n",
        "        # Generate realistic BTC price data with random walk\n",
        "        np.random.seed(42)\n",
        "        initial_price = 45000  # Starting BTC price\n",
        "\n",
        "        # Generate returns with some autocorrelation\n",
        "        returns = np.random.normal(0, 0.02, periods)  # 2% volatility\n",
        "        returns = np.cumsum(returns)  # Add trend\n",
        "\n",
        "        prices = initial_price * np.exp(returns)\n",
        "\n",
        "        # Generate OHLC from close prices\n",
        "        df = pd.DataFrame({\n",
        "            'datetime': dates,\n",
        "            'close': prices\n",
        "        })\n",
        "\n",
        "        # Generate realistic OHLC\n",
        "        df['high'] = df['close'] * (1 + np.abs(np.random.normal(0, 0.005, periods)))\n",
        "        df['low'] = df['close'] * (1 - np.abs(np.random.normal(0, 0.005, periods)))\n",
        "        df['open'] = df['close'].shift(1).fillna(df['close'])\n",
        "\n",
        "        # Ensure OHLC logic\n",
        "        df['high'] = np.maximum(df['high'], np.maximum(df['open'], df['close']))\n",
        "        df['low'] = np.minimum(df['low'], np.minimum(df['open'], df['close']))\n",
        "\n",
        "        # Generate volume\n",
        "        df['volume'] = np.random.lognormal(15, 1, periods)  # Realistic volume distribution\n",
        "\n",
        "        print(f\"âœ… Synthetic {interval} data generated: {len(df)} records\")\n",
        "        return df\n",
        "\n",
        "    def load_multi_timeframe_data(self):\n",
        "        \"\"\"Load data for all timeframes with fallbacks\"\"\"\n",
        "        print(\"ğŸ“Š Loading multi-timeframe data...\")\n",
        "\n",
        "        # Test API first\n",
        "        api_working = self.test_api_connection()\n",
        "\n",
        "        timeframes = ['5min', '15min', '1h']\n",
        "        data = {}\n",
        "\n",
        "        for tf in timeframes:\n",
        "            print(f\"\\nğŸ”„ Loading {tf}...\")\n",
        "\n",
        "            # Try main API first\n",
        "            if api_working:\n",
        "                df = self.fetch_data(interval=tf, outputsize=2000)\n",
        "                if df is not None:\n",
        "                    data[tf] = df\n",
        "                    continue\n",
        "\n",
        "            # Try fallback\n",
        "            print(f\"ğŸ”„ Trying fallback for {tf}...\")\n",
        "            df = self.fetch_fallback_data(interval=tf)\n",
        "            if df is not None:\n",
        "                data[tf] = df\n",
        "                continue\n",
        "\n",
        "            # Generate synthetic as last resort\n",
        "            print(f\"ğŸ² Using synthetic data for {tf}...\")\n",
        "            df = self.generate_synthetic_data(interval=tf)\n",
        "            data[tf] = df\n",
        "\n",
        "        print(f\"\\nâœ… Loaded {len(data)} timeframes\")\n",
        "        return data\n",
        "\n",
        "class SMCPatternDetector:\n",
        "    \"\"\"Enhanced Smart Money Concepts pattern detection\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_market_structure(df, window=20):\n",
        "        \"\"\"Detect market structure shifts (BOS/CHoCH)\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Higher Highs/Lower Lows detection\n",
        "        df['swing_high'] = df['high'].rolling(window, center=True).max() == df['high']\n",
        "        df['swing_low'] = df['low'].rolling(window, center=True).min() == df['low']\n",
        "\n",
        "        # Market structure state\n",
        "        df['market_structure'] = 0  # 0=ranging, 1=bullish, -1=bearish\n",
        "\n",
        "        swing_highs = df[df['swing_high']]['high'].values\n",
        "        swing_lows = df[df['swing_low']]['low'].values\n",
        "\n",
        "        if len(swing_highs) > 1 and len(swing_lows) > 1:\n",
        "            latest_high = swing_highs[-1] if len(swing_highs) > 0 else 0\n",
        "            prev_high = swing_highs[-2] if len(swing_highs) > 1 else 0\n",
        "            latest_low = swing_lows[-1] if len(swing_lows) > 0 else 0\n",
        "            prev_low = swing_lows[-2] if len(swing_lows) > 1 else 0\n",
        "\n",
        "            # Break of Structure (BOS)\n",
        "            if latest_high > prev_high and latest_low > prev_low:\n",
        "                df.loc[-window:, 'market_structure'] = 1  # Bullish BOS\n",
        "            elif latest_high < prev_high and latest_low < prev_low:\n",
        "                df.loc[-window:, 'market_structure'] = -1  # Bearish BOS\n",
        "\n",
        "        return df['market_structure'].fillna(0)\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_liquidity_sweeps(df, lookback=10):\n",
        "        \"\"\"Detect liquidity sweeps above/below previous highs/lows\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Recent highs and lows\n",
        "        df['recent_high'] = df['high'].rolling(lookback).max()\n",
        "        df['recent_low'] = df['low'].rolling(lookback).min()\n",
        "\n",
        "        # Liquidity sweep detection\n",
        "        df['liquidity_sweep'] = 0\n",
        "\n",
        "        # Sweep above recent high then reject\n",
        "        high_sweep = (df['high'] > df['recent_high'].shift(1)) & (df['close'] < df['recent_high'].shift(1))\n",
        "        df.loc[high_sweep, 'liquidity_sweep'] = -1  # Bearish sweep\n",
        "\n",
        "        # Sweep below recent low then recover\n",
        "        low_sweep = (df['low'] < df['recent_low'].shift(1)) & (df['close'] > df['recent_low'].shift(1))\n",
        "        df.loc[low_sweep, 'liquidity_sweep'] = 1  # Bullish sweep\n",
        "\n",
        "        return df['liquidity_sweep'].fillna(0)\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_fair_value_gaps(df, min_gap_percent=0.1):\n",
        "        \"\"\"Detect Fair Value Gaps (FVG)\"\"\"\n",
        "        df = df.copy()\n",
        "        df['fvg_signal'] = 0\n",
        "\n",
        "        for i in range(2, len(df)):\n",
        "            # Bullish FVG: gap between candle[i-2].low and candle[i].high\n",
        "            if (df.iloc[i-2]['low'] > df.iloc[i]['high'] * (1 + min_gap_percent/100) and\n",
        "                df.iloc[i-1]['close'] > df.iloc[i-1]['open']):  # Middle candle bullish\n",
        "                df.iloc[i, df.columns.get_loc('fvg_signal')] = 1\n",
        "\n",
        "            # Bearish FVG: gap between candle[i-2].high and candle[i].low\n",
        "            elif (df.iloc[i-2]['high'] < df.iloc[i]['low'] * (1 - min_gap_percent/100) and\n",
        "                  df.iloc[i-1]['close'] < df.iloc[i-1]['open']):  # Middle candle bearish\n",
        "                df.iloc[i, df.columns.get_loc('fvg_signal')] = -1\n",
        "\n",
        "        return df['fvg_signal'].fillna(0)\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_order_blocks(df, window=5):\n",
        "        \"\"\"Detect institutional order blocks\"\"\"\n",
        "        df = df.copy()\n",
        "        df['order_block'] = 0\n",
        "\n",
        "        # Bullish order block: strong bullish move after consolidation\n",
        "        bullish_move = (df['close'] / df['open'] - 1) > 0.01  # 1% move\n",
        "        recent_range = df['high'].rolling(window).max() - df['low'].rolling(window).min()\n",
        "        low_volatility = recent_range < df['close'] * 0.005  # Low volatility before move\n",
        "\n",
        "        bullish_ob = bullish_move & low_volatility.shift(1)\n",
        "        df.loc[bullish_ob, 'order_block'] = 1\n",
        "\n",
        "        # Bearish order block: strong bearish move after consolidation\n",
        "        bearish_move = (df['open'] / df['close'] - 1) > 0.01  # 1% move down\n",
        "        bearish_ob = bearish_move & low_volatility.shift(1)\n",
        "        df.loc[bearish_ob, 'order_block'] = -1\n",
        "\n",
        "        return df['order_block'].fillna(0)\n",
        "\n",
        "class AdvancedFeatureEngineering:\n",
        "    \"\"\"Advanced feature engineering for multi-timeframe analysis\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def add_technical_indicators(df):\n",
        "        \"\"\"Add comprehensive technical indicators\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Price-based features\n",
        "        df['returns'] = df['close'].pct_change()\n",
        "        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
        "        df['price_range'] = (df['high'] - df['low']) / df['close']\n",
        "        df['body_size'] = abs(df['close'] - df['open']) / df['close']\n",
        "        df['upper_shadow'] = (df['high'] - np.maximum(df['open'], df['close'])) / df['close']\n",
        "        df['lower_shadow'] = (np.minimum(df['open'], df['close']) - df['low']) / df['close']\n",
        "\n",
        "        # Volume features\n",
        "        df['volume_sma'] = df['volume'].rolling(20).mean()\n",
        "        df['volume_ratio'] = df['volume'] / df['volume_sma']\n",
        "        df['price_volume'] = df['close'] * df['volume']\n",
        "        df['vwap'] = df['price_volume'].rolling(20).sum() / df['volume'].rolling(20).sum()\n",
        "\n",
        "        # Volatility features\n",
        "        df['volatility'] = df['returns'].rolling(20).std()\n",
        "        df['atr'] = ((df['high'] - df['low']).rolling(14).mean())\n",
        "\n",
        "        # Momentum indicators\n",
        "        df['rsi'] = calculate_rsi(df['close'], 14)\n",
        "        df['macd'], df['macd_signal'] = calculate_macd(df['close'])\n",
        "        df['bb_upper'], df['bb_lower'] = calculate_bollinger_bands(df['close'])\n",
        "\n",
        "        # Moving averages\n",
        "        for period in [5, 10, 20, 50]:\n",
        "            df[f'sma_{period}'] = df['close'].rolling(period).mean()\n",
        "            df[f'ema_{period}'] = df['close'].ewm(period).mean()\n",
        "            df[f'price_sma_{period}_ratio'] = df['close'] / df[f'sma_{period}']\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def add_smc_features(df):\n",
        "        \"\"\"Add Smart Money Concepts features\"\"\"\n",
        "        smc = SMCPatternDetector()\n",
        "\n",
        "        df['market_structure'] = smc.detect_market_structure(df)\n",
        "        df['liquidity_sweep'] = smc.detect_liquidity_sweeps(df)\n",
        "        df['fvg_signal'] = smc.detect_fair_value_gaps(df)\n",
        "        df['order_block'] = smc.detect_order_blocks(df)\n",
        "\n",
        "        # Displacement detection\n",
        "        df['displacement'] = 0\n",
        "        strong_moves = abs(df['returns']) > df['returns'].rolling(50).std() * 2\n",
        "        df.loc[strong_moves, 'displacement'] = np.sign(df.loc[strong_moves, 'returns'])\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def create_multi_timeframe_features(data_dict, sequence_length=64):\n",
        "        \"\"\"Create features from multiple timeframes\"\"\"\n",
        "        print(\"ğŸ”§ Creating multi-timeframe features...\")\n",
        "\n",
        "        # Process each timeframe\n",
        "        processed_data = {}\n",
        "        for tf, df in data_dict.items():\n",
        "            print(f\"Processing {tf}...\")\n",
        "\n",
        "            # Add technical indicators\n",
        "            df = AdvancedFeatureEngineering.add_technical_indicators(df)\n",
        "            df = AdvancedFeatureEngineering.add_smc_features(df)\n",
        "\n",
        "            # Clean data\n",
        "            df = df.fillna(method='ffill').fillna(method='bfill')\n",
        "            df = df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "            processed_data[tf] = df\n",
        "\n",
        "        # Align timeframes (use 15min as base)\n",
        "        base_df = processed_data['15min'].copy()\n",
        "\n",
        "        # Add higher timeframe context (1h)\n",
        "        if '1h' in processed_data:\n",
        "            hourly_df = processed_data['1h'].copy()\n",
        "            hourly_df['datetime'] = pd.to_datetime(hourly_df['datetime'])\n",
        "            hourly_df = hourly_df.set_index('datetime')\n",
        "\n",
        "            # Resample to 15min and forward fill\n",
        "            hourly_resampled = hourly_df.resample('15min').ffill()\n",
        "\n",
        "            # Merge with base timeframe\n",
        "            base_df['datetime'] = pd.to_datetime(base_df['datetime'])\n",
        "            base_df = base_df.set_index('datetime')\n",
        "\n",
        "            # Add hourly features with prefix\n",
        "            hourly_features = ['market_structure', 'rsi', 'macd', 'volatility', 'price_sma_20_ratio']\n",
        "            for feature in hourly_features:\n",
        "                if feature in hourly_resampled.columns:\n",
        "                    base_df[f'h1_{feature}'] = hourly_resampled[feature]\n",
        "\n",
        "        # Add lower timeframe signals (5min)\n",
        "        if '5min' in processed_data:\n",
        "            min5_df = processed_data['5min'].copy()\n",
        "            min5_df['datetime'] = pd.to_datetime(min5_df['datetime'])\n",
        "            min5_df = min5_df.set_index('datetime')\n",
        "\n",
        "            # Aggregate 5min signals to 15min\n",
        "            min5_agg = min5_df.resample('15min').agg({\n",
        "                'liquidity_sweep': 'sum',\n",
        "                'fvg_signal': 'sum',\n",
        "                'displacement': 'sum',\n",
        "                'volatility': 'mean',\n",
        "                'volume_ratio': 'mean'\n",
        "            })\n",
        "\n",
        "            # Add 5min features with prefix\n",
        "            for col in min5_agg.columns:\n",
        "                base_df[f'm5_{col}'] = min5_agg[col]\n",
        "\n",
        "        base_df = base_df.reset_index()\n",
        "        base_df = base_df.fillna(method='ffill').fillna(0)\n",
        "\n",
        "        print(f\"âœ… Multi-timeframe features created: {base_df.shape}\")\n",
        "        return base_df\n",
        "\n",
        "class EnsembleTradingModel:\n",
        "    \"\"\"Ensemble model combining LSTM and XGBoost\"\"\"\n",
        "\n",
        "    def __init__(self, sequence_length=64, save_path=None):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.save_path = save_path\n",
        "        self.lstm_model = None\n",
        "        self.xgb_model = None\n",
        "        self.scalers = {}\n",
        "        self.feature_columns = None\n",
        "\n",
        "    def load_base_model(self):\n",
        "        \"\"\"Load existing v2.0 model as foundation\"\"\"\n",
        "        model_path = os.path.join(self.save_path, 'best_btc_model_v2.keras')\n",
        "\n",
        "        if os.path.exists(model_path):\n",
        "            print(\"ğŸ“¥ Loading base model v2.0...\")\n",
        "            try:\n",
        "                self.base_model = load_model(model_path)\n",
        "                print(\"âœ… Base model v2.0 loaded successfully\")\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error loading base model: {e}\")\n",
        "                return False\n",
        "        else:\n",
        "            print(\"âš ï¸ Base model v2.0 not found, will create new model\")\n",
        "            return False\n",
        "\n",
        "    def create_enhanced_lstm(self, input_shape_15m, input_shape_context=None):\n",
        "        \"\"\"Create enhanced LSTM with multi-timeframe inputs\"\"\"\n",
        "        print(\"ğŸ§  Creating enhanced LSTM architecture...\")\n",
        "\n",
        "        # Main 15min input\n",
        "        input_15m = Input(shape=input_shape_15m, name='input_15m')\n",
        "\n",
        "        # LSTM layers for main timeframe (based on v2.0 architecture)\n",
        "        lstm1 = LSTM(32, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)(input_15m)\n",
        "        lstm1 = BatchNormalization()(lstm1)\n",
        "\n",
        "        lstm2 = LSTM(16, return_sequences=False, dropout=0.3, recurrent_dropout=0.3)(lstm1)\n",
        "        lstm2 = BatchNormalization()(lstm2)\n",
        "\n",
        "        # If we have context features (1h, 5m aggregated)\n",
        "        if input_shape_context is not None:\n",
        "            context_input = Input(shape=(input_shape_context,), name='context_input')\n",
        "            context_dense = Dense(8, activation='relu')(context_input)\n",
        "            context_dense = Dropout(0.3)(context_dense)\n",
        "\n",
        "            # Combine LSTM output with context\n",
        "            combined = Concatenate()([lstm2, context_dense])\n",
        "        else:\n",
        "            combined = lstm2\n",
        "\n",
        "        # Final layers (based on successful v2.0 architecture)\n",
        "        dense1 = Dense(16, activation='relu')(combined)\n",
        "        dense1 = BatchNormalization()(dense1)\n",
        "        dense1 = Dropout(0.5)(dense1)\n",
        "\n",
        "        output = Dense(3, activation='softmax', name='prediction')(dense1)  # Buy, Hold, Sell\n",
        "\n",
        "        # Create model\n",
        "        if input_shape_context is not None:\n",
        "            model = Model(inputs=[input_15m, context_input], outputs=output)\n",
        "        else:\n",
        "            model = Model(inputs=input_15m, outputs=output)\n",
        "\n",
        "        # Use successful v2.0 optimizer settings\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def prepare_sequences(self, df, target_col='target'):\n",
        "        \"\"\"Prepare sequences for LSTM with memory efficiency\"\"\"\n",
        "        print(\"ğŸ“Š Preparing sequences...\")\n",
        "\n",
        "        # Select features (excluding non-numeric columns)\n",
        "        feature_cols = [col for col in df.columns if col not in\n",
        "                       ['datetime', 'target', 'signal', 'future_return']]\n",
        "\n",
        "        self.feature_columns = feature_cols\n",
        "\n",
        "        # Prepare data\n",
        "        X_data = df[feature_cols].values\n",
        "        y_data = df[target_col].values if target_col in df.columns else None\n",
        "\n",
        "        # Scale features\n",
        "        self.scalers['features'] = StandardScaler()\n",
        "        X_scaled = self.scalers['features'].fit_transform(X_data)\n",
        "\n",
        "        # Create sequences\n",
        "        X_sequences = []\n",
        "        y_sequences = []\n",
        "\n",
        "        for i in range(self.sequence_length, len(X_scaled)):\n",
        "            X_sequences.append(X_scaled[i-self.sequence_length:i])\n",
        "            if y_data is not None:\n",
        "                y_sequences.append(y_data[i])\n",
        "\n",
        "        X_sequences = np.array(X_sequences)\n",
        "\n",
        "        if y_data is not None:\n",
        "            y_sequences = np.array(y_sequences)\n",
        "            print(f\"âœ… Sequences prepared: {X_sequences.shape}, targets: {y_sequences.shape}\")\n",
        "            return X_sequences, y_sequences\n",
        "        else:\n",
        "            print(f\"âœ… Sequences prepared: {X_sequences.shape}\")\n",
        "            return X_sequences, None\n",
        "\n",
        "    def create_trading_signals(self, df, profit_threshold=0.004):\n",
        "        \"\"\"Create trading signals based on future returns\"\"\"\n",
        "        print(f\"ğŸ¯ Creating trading signals (threshold: {profit_threshold*100}%)...\")\n",
        "\n",
        "        # Calculate future returns (15 periods ahead for 15min = ~4 hours)\n",
        "        df['future_return'] = df['close'].shift(-15) / df['close'] - 1\n",
        "\n",
        "        # Create targets based on v2.0 successful thresholds\n",
        "        conditions = [\n",
        "            df['future_return'] > profit_threshold,    # Buy signal\n",
        "            df['future_return'] < -profit_threshold,   # Sell signal\n",
        "        ]\n",
        "        choices = [1, 2]  # 1=Buy, 2=Sell\n",
        "        df['signal'] = np.select(conditions, choices, default=0)  # 0=Hold\n",
        "\n",
        "        # Convert to categorical\n",
        "        from tensorflow.keras.utils import to_categorical\n",
        "        df['target'] = df['signal']\n",
        "        y_categorical = to_categorical(df['signal'], num_classes=3)\n",
        "\n",
        "        # Add categorical columns to dataframe\n",
        "        df['target_0'] = y_categorical[:, 0]  # Hold\n",
        "        df['target_1'] = y_categorical[:, 1]  # Buy\n",
        "        df['target_2'] = y_categorical[:, 2]  # Sell\n",
        "\n",
        "        signal_counts = df['signal'].value_counts()\n",
        "        print(f\"Signal distribution: {dict(signal_counts)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def train_ensemble(self, df):\n",
        "        \"\"\"Train ensemble model with fine-tuning approach\"\"\"\n",
        "        print(\"ğŸš€ Training ensemble model...\")\n",
        "\n",
        "        # Create trading signals\n",
        "        df = self.create_trading_signals(df)\n",
        "\n",
        "        # Remove rows with NaN targets\n",
        "        df = df.dropna(subset=['target'])\n",
        "\n",
        "        # Prepare sequences\n",
        "        X_sequences, y_sequences = self.prepare_sequences(df, 'target')\n",
        "\n",
        "        # Train/validation split (80/20 based on v2.0 success)\n",
        "        split_idx = int(len(X_sequences) * 0.8)\n",
        "\n",
        "        X_train = X_sequences[:split_idx]\n",
        "        y_train = y_sequences[:split_idx]\n",
        "        X_val = X_sequences[split_idx:]\n",
        "        y_val = y_sequences[split_idx:]\n",
        "\n",
        "        # Convert targets to categorical\n",
        "        from tensorflow.keras.utils import to_categorical\n",
        "        y_train_cat = to_categorical(y_train, num_classes=3)\n",
        "        y_val_cat = to_categorical(y_val, num_classes=3)\n",
        "\n",
        "        print(f\"Training data: {X_train.shape}, Validation: {X_val.shape}\")\n",
        "\n",
        "        # Create enhanced LSTM\n",
        "        self.lstm_model = self.create_enhanced_lstm(\n",
        "            input_shape_15m=(self.sequence_length, X_train.shape[2])\n",
        "        )\n",
        "\n",
        "        # Callbacks (based on v2.0 successful settings)\n",
        "        callbacks = [\n",
        "            EarlyStopping(patience=15, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(factor=0.5, patience=10, min_lr=0.0001)\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        print(\"ğŸ“ Training enhanced LSTM...\")\n",
        "        history = self.lstm_model.fit(\n",
        "            X_train, y_train_cat,\n",
        "            validation_data=(X_val, y_val_cat),\n",
        "            epochs=100,\n",
        "            batch_size=16,  # v2.0 successful batch size\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        val_pred = self.lstm_model.predict(X_val)\n",
        "        val_pred_classes = np.argmax(val_pred, axis=1)\n",
        "        val_accuracy = accuracy_score(y_val, val_pred_classes)\n",
        "\n",
        "        print(f\"âœ… LSTM Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Train XGBoost on recent features (memory efficient)\n",
        "        print(\"ğŸŒ² Training XGBoost...\")\n",
        "        try:\n",
        "            import xgboost as xgb\n",
        "\n",
        "            # Use last 2000 samples for XGBoost (memory efficient)\n",
        "            recent_data = df.tail(2000).copy()\n",
        "            feature_cols = [col for col in recent_data.columns if col not in\n",
        "                           ['datetime', 'target', 'signal', 'future_return', 'target_0', 'target_1', 'target_2']]\n",
        "\n",
        "            X_xgb = recent_data[feature_cols].fillna(0)\n",
        "            y_xgb = recent_data['target'].values\n",
        "\n",
        "            # Scale features for XGBoost\n",
        "            self.scalers['xgb'] = StandardScaler()\n",
        "            X_xgb_scaled = self.scalers['xgb'].fit_transform(X_xgb)\n",
        "\n",
        "            # Train XGBoost\n",
        "            self.xgb_model = xgb.XGBClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=6,\n",
        "                learning_rate=0.1,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            # Train/val split for XGBoost\n",
        "            split_xgb = int(len(X_xgb_scaled) * 0.8)\n",
        "            self.xgb_model.fit(\n",
        "                X_xgb_scaled[:split_xgb],\n",
        "                y_xgb[:split_xgb]\n",
        "            )\n",
        "\n",
        "            # Evaluate XGBoost\n",
        "            xgb_pred = self.xgb_model.predict(X_xgb_scaled[split_xgb:])\n",
        "            xgb_accuracy = accuracy_score(y_xgb[split_xgb:], xgb_pred)\n",
        "            print(f\"âœ… XGBoost Accuracy: {xgb_accuracy:.4f}\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"âš ï¸ XGBoost not available, using LSTM only\")\n",
        "            self.xgb_model = None\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict_ensemble(self, X_sequences, X_features=None):\n",
        "        \"\"\"Make ensemble predictions\"\"\"\n",
        "        # LSTM predictions\n",
        "        lstm_pred = self.lstm_model.predict(X_sequences)\n",
        "\n",
        "        # XGBoost predictions (if available)\n",
        "        if self.xgb_model is not None and X_features is not None:\n",
        "            xgb_pred_proba = self.xgb_model.predict_proba(X_features)\n",
        "\n",
        "            # Ensure same number of classes\n",
        "            if xgb_pred_proba.shape[1] == 3:\n",
        "                # Ensemble averaging\n",
        "                ensemble_pred = 0.7 * lstm_pred + 0.3 * xgb_pred_proba\n",
        "            else:\n",
        "                ensemble_pred = lstm_pred\n",
        "        else:\n",
        "            ensemble_pred = lstm_pred\n",
        "\n",
        "        return ensemble_pred\n",
        "\n",
        "    def save_models(self):\n",
        "        \"\"\"Save all model components\"\"\"\n",
        "        if self.save_path is None:\n",
        "            return\n",
        "\n",
        "        print(\"ğŸ’¾ Saving model components...\")\n",
        "\n",
        "        # Save LSTM model\n",
        "        lstm_path = os.path.join(self.save_path, 'btc_lstm_v3_multitimeframe.keras')\n",
        "        self.lstm_model.save(lstm_path)\n",
        "        print(f\"âœ… LSTM saved: {lstm_path}\")\n",
        "\n",
        "        # Save ensemble components\n",
        "        if self.xgb_model is not None:\n",
        "            ensemble_path = os.path.join(self.save_path, 'btc_ensemble_v3.pkl')\n",
        "            joblib.dump(self.xgb_model, ensemble_path)\n",
        "            print(f\"âœ… XGBoost saved: {ensemble_path}\")\n",
        "\n",
        "        # Save scalers\n",
        "        scalers_path = os.path.join(self.save_path, 'btc_scalers_v3.pkl')\n",
        "        joblib.dump(self.scalers, scalers_path)\n",
        "        print(f\"âœ… Scalers saved: {scalers_path}\")\n",
        "\n",
        "class MemoryEfficientBacktester:\n",
        "    \"\"\"Memory efficient backtesting with chunked processing\"\"\"\n",
        "\n",
        "    def __init__(self, initial_capital=10000, transaction_cost=0.001):\n",
        "        self.initial_capital = initial_capital\n",
        "        self.transaction_cost = transaction_cost\n",
        "        self.results = []\n",
        "\n",
        "    def backtest_chunked(self, model, df, chunk_size=1000):\n",
        "        \"\"\"Perform chunked backtesting to save memory\"\"\"\n",
        "        print(\"ğŸ“ˆ Running memory-efficient backtest...\")\n",
        "\n",
        "        total_chunks = len(df) // chunk_size + 1\n",
        "        all_results = []\n",
        "\n",
        "        for i in range(0, len(df), chunk_size):\n",
        "            chunk = df.iloc[i:i+chunk_size].copy()\n",
        "            if len(chunk) < model.sequence_length:\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing chunk {i//chunk_size + 1}/{total_chunks}\")\n",
        "\n",
        "            # Prepare sequences for this chunk\n",
        "            X_sequences, _ = model.prepare_sequences(chunk)\n",
        "\n",
        "            if len(X_sequences) == 0:\n",
        "                continue\n",
        "\n",
        "            # Make predictions\n",
        "            predictions = model.lstm_model.predict(X_sequences, verbose=0)\n",
        "            pred_classes = np.argmax(predictions, axis=1)\n",
        "            pred_confidence = np.max(predictions, axis=1)\n",
        "\n",
        "            # Create results for this chunk\n",
        "            chunk_results = pd.DataFrame({\n",
        "                'datetime': chunk['datetime'].iloc[model.sequence_length:],\n",
        "                'close': chunk['close'].iloc[model.sequence_length:],\n",
        "                'prediction': pred_classes,\n",
        "                'confidence': pred_confidence,\n",
        "                'actual_signal': chunk['signal'].iloc[model.sequence_length:] if 'signal' in chunk.columns else 0\n",
        "            })\n",
        "\n",
        "            all_results.append(chunk_results)\n",
        "\n",
        "            # Clean memory\n",
        "            del X_sequences, predictions, chunk\n",
        "            gc.collect()\n",
        "\n",
        "        # Combine all results\n",
        "        results_df = pd.concat(all_results, ignore_index=True)\n",
        "        return self.calculate_performance(results_df)\n",
        "\n",
        "    def calculate_performance(self, results_df, confidence_threshold=0.7):\n",
        "        \"\"\"Calculate detailed performance metrics\"\"\"\n",
        "        print(\"ğŸ“Š Calculating performance metrics...\")\n",
        "\n",
        "        # Filter by confidence (v2.0 successful approach)\n",
        "        high_conf = results_df[results_df['confidence'] >= confidence_threshold].copy()\n",
        "\n",
        "        if len(high_conf) == 0:\n",
        "            print(\"âš ï¸ No high confidence predictions\")\n",
        "            return {}\n",
        "\n",
        "        # Simulate trading\n",
        "        capital = self.initial_capital\n",
        "        position = 0  # 0=no position, 1=long, -1=short\n",
        "        trades = []\n",
        "        equity_curve = [capital]\n",
        "\n",
        "        for i, row in high_conf.iterrows():\n",
        "            signal = row['prediction']\n",
        "            price = row['close']\n",
        "\n",
        "            # Trading logic based on v2.0 success\n",
        "            if signal == 1 and position <= 0:  # Buy signal\n",
        "                if position == -1:  # Close short\n",
        "                    pnl = (entry_price - price) / entry_price * capital * 0.02  # 2% risk\n",
        "                    capital += pnl * (1 - self.transaction_cost)\n",
        "                    trades.append({'type': 'close_short', 'price': price, 'pnl': pnl})\n",
        "\n",
        "                # Open long\n",
        "                position = 1\n",
        "                entry_price = price\n",
        "                trades.append({'type': 'buy', 'price': price})\n",
        "\n",
        "            elif signal == 2 and position >= 0:  # Sell signal\n",
        "                if position == 1:  # Close long\n",
        "                    pnl = (price - entry_price) / entry_price * capital * 0.02  # 2% risk\n",
        "                    capital += pnl * (1 - self.transaction_cost)\n",
        "                    trades.append({'type': 'close_long', 'price': price, 'pnl': pnl})\n",
        "\n",
        "                # Open short\n",
        "                position = -1\n",
        "                entry_price = price\n",
        "                trades.append({'type': 'sell', 'price': price})\n",
        "\n",
        "            equity_curve.append(capital)\n",
        "\n",
        "        # Calculate metrics\n",
        "        total_return = (capital - self.initial_capital) / self.initial_capital\n",
        "\n",
        "        if len(trades) > 0:\n",
        "            pnl_trades = [t['pnl'] for t in trades if 'pnl' in t]\n",
        "            if pnl_trades:\n",
        "                win_rate = len([p for p in pnl_trades if p > 0]) / len(pnl_trades)\n",
        "                avg_win = np.mean([p for p in pnl_trades if p > 0]) if any(p > 0 for p in pnl_trades) else 0\n",
        "                avg_loss = np.mean([p for p in pnl_trades if p < 0]) if any(p < 0 for p in pnl_trades) else 0\n",
        "            else:\n",
        "                win_rate = 0\n",
        "                avg_win = 0\n",
        "                avg_loss = 0\n",
        "        else:\n",
        "            win_rate = 0\n",
        "            avg_win = 0\n",
        "            avg_loss = 0\n",
        "\n",
        "        # Sharpe ratio\n",
        "        if len(equity_curve) > 1:\n",
        "            returns = np.diff(equity_curve) / equity_curve[:-1]\n",
        "            sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252) if np.std(returns) > 0 else 0\n",
        "        else:\n",
        "            sharpe_ratio = 0\n",
        "\n",
        "        # Maximum drawdown\n",
        "        equity_series = pd.Series(equity_curve)\n",
        "        rolling_max = equity_series.expanding().max()\n",
        "        drawdown = (equity_series - rolling_max) / rolling_max\n",
        "        max_drawdown = drawdown.min()\n",
        "\n",
        "        # Accuracy vs actual signals\n",
        "        if 'actual_signal' in high_conf.columns:\n",
        "            accuracy = accuracy_score(high_conf['actual_signal'], high_conf['prediction'])\n",
        "        else:\n",
        "            accuracy = 0\n",
        "\n",
        "        metrics = {\n",
        "            'total_return': total_return,\n",
        "            'accuracy': accuracy,\n",
        "            'win_rate': win_rate,\n",
        "            'sharpe_ratio': sharpe_ratio,\n",
        "            'max_drawdown': max_drawdown,\n",
        "            'num_trades': len(trades),\n",
        "            'avg_win': avg_win,\n",
        "            'avg_loss': avg_loss,\n",
        "            'final_capital': capital,\n",
        "            'high_confidence_signals': len(high_conf)\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "# Helper functions for technical indicators\n",
        "def calculate_rsi(prices, period=14):\n",
        "    \"\"\"Calculate RSI\"\"\"\n",
        "    delta = prices.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi.fillna(50)\n",
        "\n",
        "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
        "    \"\"\"Calculate MACD\"\"\"\n",
        "    ema_fast = prices.ewm(span=fast).mean()\n",
        "    ema_slow = prices.ewm(span=slow).mean()\n",
        "    macd = ema_fast - ema_slow\n",
        "    macd_signal = macd.ewm(span=signal).mean()\n",
        "    return macd.fillna(0), macd_signal.fillna(0)\n",
        "\n",
        "def calculate_bollinger_bands(prices, period=20, std_dev=2):\n",
        "    \"\"\"Calculate Bollinger Bands\"\"\"\n",
        "    sma = prices.rolling(window=period).mean()\n",
        "    std = prices.rolling(window=period).std()\n",
        "    upper_band = sma + (std * std_dev)\n",
        "    lower_band = sma - (std * std_dev)\n",
        "    return upper_band.fillna(sma), lower_band.fillna(sma)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline\"\"\"\n",
        "    print(\"ğŸš€ BTC LSTM Trading Model v3.0 - Multi-Timeframe Ensemble\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize components\n",
        "    data_loader = BTCDataLoader(API_KEY)\n",
        "\n",
        "    try:\n",
        "        # Load multi-timeframe data\n",
        "        data_dict = data_loader.load_multi_timeframe_data()\n",
        "\n",
        "        if not data_dict:\n",
        "            print(\"âŒ Failed to load data\")\n",
        "            return\n",
        "\n",
        "        # Create multi-timeframe features\n",
        "        df = AdvancedFeatureEngineering.create_multi_timeframe_features(data_dict)\n",
        "\n",
        "        print(f\"ğŸ“Š Final dataset shape: {df.shape}\")\n",
        "        print(f\"Features: {df.columns.tolist()}\")\n",
        "\n",
        "        # Initialize ensemble model\n",
        "        model = EnsembleTradingModel(sequence_length=64, save_path=SAVE_PATH)\n",
        "\n",
        "        # Try to load base model v2.0\n",
        "        model.load_base_model()\n",
        "\n",
        "        # Train enhanced model\n",
        "        history = model.train_ensemble(df)\n",
        "\n",
        "        # Save models\n",
        "        model.save_models()\n",
        "\n",
        "        # Memory efficient backtesting\n",
        "        backtester = MemoryEfficientBacktester()\n",
        "        metrics = backtester.backtest_chunked(model, df)\n",
        "\n",
        "        # Display results\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"ğŸ“ˆ BACKTEST RESULTS - BTC LSTM v3.0\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        print(f\"ğŸ’° Total Return: {metrics.get('total_return', 0)*100:.2f}%\")\n",
        "        print(f\"ğŸ¯ Accuracy: {metrics.get('accuracy', 0)*100:.2f}%\")\n",
        "        print(f\"âœ… Win Rate: {metrics.get('win_rate', 0)*100:.2f}%\")\n",
        "        print(f\"ğŸ“Š Sharpe Ratio: {metrics.get('sharpe_ratio', 0):.3f}\")\n",
        "        print(f\"ğŸ“‰ Max Drawdown: {metrics.get('max_drawdown', 0)*100:.2f}%\")\n",
        "        print(f\"ğŸ”„ Number of Trades: {metrics.get('num_trades', 0)}\")\n",
        "        print(f\"ğŸ’µ Final Capital: ${metrics.get('final_capital', 0):,.2f}\")\n",
        "\n",
        "        # Compare with v2.0 benchmarks\n",
        "        print(\"\\n\" + \"=\" * 40)\n",
        "        print(\"ğŸ“Š COMPARISON WITH v2.0\")\n",
        "        print(\"=\" * 40)\n",
        "        print(\"v2.0 Results: 66% accuracy, +0.2% return, 0.21 Sharpe\")\n",
        "        print(f\"v3.0 Results: {metrics.get('accuracy', 0)*100:.1f}% accuracy, {metrics.get('total_return', 0)*100:+.1f}% return, {metrics.get('sharpe_ratio', 0):.2f} Sharpe\")\n",
        "\n",
        "        improvement = metrics.get('total_return', 0) - 0.002  # v2.0 had +0.2%\n",
        "        print(f\"ğŸš€ Improvement: {improvement*100:+.2f}% return\")\n",
        "\n",
        "        # Save detailed results\n",
        "        results_path = os.path.join(SAVE_PATH, 'btc_v3_results.csv')\n",
        "        results_df = pd.DataFrame([metrics])\n",
        "        results_df.to_csv(results_path, index=False)\n",
        "        print(f\"ğŸ’¾ Results saved: {results_path}\")\n",
        "\n",
        "        print(\"\\nâœ… BTC LSTM v3.0 training and evaluation completed!\")\n",
        "\n",
        "        # Memory cleanup\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error in main execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# REAL-TIME PREDICTION Ğ¤Ğ£ĞĞšĞ¦Ğ˜Ğ¯\n",
        "# ===============================\n",
        "\n",
        "def predict_now():\n",
        "    \"\"\"Real-time prediction Ñ ÑĞ²ĞµĞ¶Ğ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸\"\"\"\n",
        "    print(\"ğŸ”® REAL-TIME BTC PREDICTION\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸ scalers\n",
        "    try:\n",
        "        model_path = os.path.join(SAVE_PATH, 'btc_lstm_v3_multitimeframe.keras')\n",
        "        scalers_path = os.path.join(SAVE_PATH, 'btc_scalers_v3.pkl')\n",
        "\n",
        "        from tensorflow.keras.models import load_model\n",
        "        lstm_model = load_model(model_path)\n",
        "        scalers = joblib.load(scalers_path)\n",
        "        print(\"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ v3.0 Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°\")\n",
        "\n",
        "        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ ÑĞ²ĞµĞ¶Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ\n",
        "        data_loader = BTCDataLoader(API_KEY)\n",
        "        df = data_loader.fetch_data('BTCUSD', '15min', outputsize=200)\n",
        "\n",
        "        if df is None:\n",
        "            df = data_loader.fetch_fallback_data('15min')\n",
        "\n",
        "        print(f\"ğŸ“Š Ğ¡Ğ²ĞµĞ¶Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ: {len(df)} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹\")\n",
        "        print(f\"ğŸ• ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½ÑÑ ÑĞ²ĞµÑ‡ĞºĞ°: {df['datetime'].iloc[-1]}\")\n",
        "        print(f\"ğŸ’° Ğ¢ĞµĞºÑƒÑ‰Ğ°Ñ Ñ†ĞµĞ½Ğ° BTC: ${df['close'].iloc[-1]:,.2f}\")\n",
        "\n",
        "        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ features\n",
        "        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ multi-timeframe features ĞºĞ°Ğº Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸\n",
        "        data_dict = {'15min': df}\n",
        "\n",
        "        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ 1h ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚\n",
        "        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ 1h ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ (Ñ fallback)\n",
        "        hourly_df = data_loader.fetch_data('BTC/USD', '1h', outputsize=100)\n",
        "        if hourly_df is None:\n",
        "            hourly_df = data_loader.fetch_fallback_data('1h')\n",
        "        if hourly_df is None:\n",
        "            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¸Ğ· 15min Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…\n",
        "            hourly_df = df.resample('1h', on='datetime').agg({\n",
        "                'open': 'first', 'high': 'max', 'low': 'min',\n",
        "                'close': 'last', 'volume': 'sum'\n",
        "            }).reset_index()\n",
        "        data_dict['1h'] = hourly_df\n",
        "\n",
        "        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ 5min ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ (Ñ fallback)\n",
        "        min5_df = data_loader.fetch_data('BTC/USD', '5min', outputsize=100)\n",
        "        if min5_df is None:\n",
        "            min5_df = data_loader.fetch_fallback_data('5min')\n",
        "        if min5_df is None:\n",
        "            # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ synthetic 5min Ğ¸Ğ· 15min\n",
        "            min5_df = data_loader.generate_synthetic_data('5min', 100)\n",
        "        data_dict['5min'] = min5_df\n",
        "\n",
        "        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ‚Ğµ Ğ¶Ğµ features Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸\n",
        "        df = AdvancedFeatureEngineering.create_multi_timeframe_features(data_dict)\n",
        "        df = df.fillna(method='ffill').fillna(0)\n",
        "\n",
        "        # Ğ‘ĞµÑ€ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 64 ÑĞ²ĞµÑ‡ĞºĞ¸\n",
        "        # Ğ‘ĞµÑ€ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 64 ÑĞ²ĞµÑ‡ĞºĞ¸\n",
        "        feature_cols = [col for col in df.columns if col not in\n",
        "                      ['datetime', 'target', 'signal', 'future_return']]\n",
        "\n",
        "        X_data = df[feature_cols].tail(64).values\n",
        "\n",
        "        # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ´Ğ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ° features (52)\n",
        "        current_features = X_data.shape[1]\n",
        "        expected_features = 52\n",
        "\n",
        "        if current_features < expected_features:\n",
        "            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ğµ features Ñ Ğ½ÑƒĞ»ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑĞ¼Ğ¸\n",
        "            missing_features = expected_features - current_features\n",
        "            padding = np.zeros((X_data.shape[0], missing_features))\n",
        "            X_data = np.hstack([X_data, padding])\n",
        "            print(f\"ğŸ”§ Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ {missing_features} features: {current_features} â†’ {expected_features}\")\n",
        "        elif current_features > expected_features:\n",
        "            # ĞĞ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ features\n",
        "            X_data = X_data[:, :expected_features]\n",
        "            print(f\"ğŸ”§ ĞĞ±Ñ€ĞµĞ·Ğ°Ğ½Ğ¾ features: {current_features} â†’ {expected_features}\")\n",
        "        X_scaled = scalers['features'].transform(X_data)\n",
        "        X_sequence = X_scaled.reshape(1, 64, -1)\n",
        "\n",
        "        # Prediction\n",
        "        prediction = lstm_model.predict(X_sequence, verbose=0)[0]\n",
        "        pred_class = np.argmax(prediction)\n",
        "        confidence = np.max(prediction)\n",
        "\n",
        "        # Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚\n",
        "        signals = {0: \"HOLD ğŸ“Š\", 1: \"BUY ğŸŸ¢\", 2: \"SELL ğŸ”´\"}\n",
        "        signal_name = signals[pred_class]\n",
        "\n",
        "        print(\"\\nğŸ¯ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢:\")\n",
        "        print(f\"ğŸ”® Ğ¡Ğ¸Ğ³Ğ½Ğ°Ğ»: {signal_name}\")\n",
        "        print(f\"ğŸ¯ Confidence: {confidence*100:.1f}%\")\n",
        "        print(f\"ğŸ“ˆ Buy: {prediction[1]*100:.1f}%\")\n",
        "        print(f\"ğŸ“Š Hold: {prediction[0]*100:.1f}%\")\n",
        "        print(f\"ğŸ“‰ Sell: {prediction[2]*100:.1f}%\")\n",
        "\n",
        "        if confidence >= 0.75:\n",
        "            print(\"âœ… Ğ’Ğ«Ğ¡ĞĞšĞĞ¯ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ!\")\n",
        "        elif confidence >= 0.6:\n",
        "            print(\"âš ï¸ Ğ¡Ğ Ğ•Ğ”ĞĞ¯Ğ¯ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ\")\n",
        "        else:\n",
        "            print(\"âŒ ĞĞ˜Ğ—ĞšĞĞ¯ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ\")\n",
        "\n",
        "        return pred_class, confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Ğ—Ğ°Ğ¿ÑƒÑĞº real-time prediction\n",
        "print(\"\\n\" + \"ğŸš€ Ğ¢Ğ•Ğ¡Ğ¢Ğ˜Ğ Ğ£Ğ•Ğœ ĞœĞĞ”Ğ•Ğ›Ğ¬ ĞĞ Ğ–Ğ˜Ğ’Ğ«Ğ¥ Ğ”ĞĞĞĞ«Ğ¥:\")\n",
        "predict_now()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QTuc_JcNLjx",
        "outputId": "0992475e-cc33-4ea2-fb9e-a4336abe608d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸš€ Ğ¢Ğ•Ğ¡Ğ¢Ğ˜Ğ Ğ£Ğ•Ğœ ĞœĞĞ”Ğ•Ğ›Ğ¬ ĞĞ Ğ–Ğ˜Ğ’Ğ«Ğ¥ Ğ”ĞĞĞĞ«Ğ¥:\n",
            "ğŸ”® REAL-TIME BTC PREDICTION\n",
            "========================================\n",
            "âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ v3.0 Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°\n",
            "ğŸ”„ Trying symbol: BTCUSD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['code', 'message', 'status']\n",
            "ğŸ“ API Message: **symbol** or **figi** parameter is missing or invalid. Please provide a valid symbol according to API documentation: https://twelvedata.com/docs#reference-data\n",
            "ğŸ”„ Trying symbol: BTC/USD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['meta', 'values', 'status']\n",
            "âœ… Raw data shape: (200, 5)\n",
            "ğŸ“‹ Columns: ['datetime', 'open', 'high', 'low', 'close']\n",
            "ğŸ“Š Created synthetic volume column\n",
            "âœ… 15min data loaded: 200 records with symbol BTC/USD\n",
            "ğŸ“Š Ğ¡Ğ²ĞµĞ¶Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ: 200 Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹\n",
            "ğŸ• ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½ÑÑ ÑĞ²ĞµÑ‡ĞºĞ°: 2025-06-20 21:15:00\n",
            "ğŸ’° Ğ¢ĞµĞºÑƒÑ‰Ğ°Ñ Ñ†ĞµĞ½Ğ° BTC: $103,528.21\n",
            "ğŸ”„ Trying symbol: BTC/USD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['meta', 'values', 'status']\n",
            "âœ… Raw data shape: (100, 5)\n",
            "ğŸ“‹ Columns: ['datetime', 'open', 'high', 'low', 'close']\n",
            "ğŸ“Š Created synthetic volume column\n",
            "âš ï¸ Too few records: 100\n",
            "ğŸ”„ Trying symbol: BTC/USD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['meta', 'values', 'status']\n",
            "âœ… Raw data shape: (100, 5)\n",
            "ğŸ“‹ Columns: ['datetime', 'open', 'high', 'low', 'close']\n",
            "ğŸ“Š Created synthetic volume column\n",
            "âš ï¸ Too few records: 100\n",
            "ğŸ”„ Trying symbol: BTCUSD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['code', 'message', 'status']\n",
            "ğŸ“ API Message: **symbol** or **figi** parameter is missing or invalid. Please provide a valid symbol according to API documentation: https://twelvedata.com/docs#reference-data\n",
            "ğŸ”„ Trying symbol: BTC-USD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['code', 'message', 'status']\n",
            "ğŸ“ API Message: **symbol** or **figi** parameter is missing or invalid. Please provide a valid symbol according to API documentation: https://twelvedata.com/docs#reference-data\n",
            "âŒ All symbol formats failed for 1h\n",
            "ğŸ”„ Trying yfinance fallback for 1h...\n",
            "âœ… Fallback 1h data loaded: 8760 records\n",
            "ğŸ”„ Trying symbol: BTC/USD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['meta', 'values', 'status']\n",
            "âœ… Raw data shape: (100, 5)\n",
            "ğŸ“‹ Columns: ['datetime', 'open', 'high', 'low', 'close']\n",
            "ğŸ“Š Created synthetic volume column\n",
            "âš ï¸ Too few records: 100\n",
            "ğŸ”„ Trying symbol: BTC/USD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['meta', 'values', 'status']\n",
            "âœ… Raw data shape: (100, 5)\n",
            "ğŸ“‹ Columns: ['datetime', 'open', 'high', 'low', 'close']\n",
            "ğŸ“Š Created synthetic volume column\n",
            "âš ï¸ Too few records: 100\n",
            "ğŸ”„ Trying symbol: BTCUSD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['code', 'message', 'status']\n",
            "ğŸ“ API Message: **symbol** or **figi** parameter is missing or invalid. Please provide a valid symbol according to API documentation: https://twelvedata.com/docs#reference-data\n",
            "ğŸ”„ Trying symbol: BTC-USD\n",
            "ğŸ“¡ Response status: 200\n",
            "ğŸ“„ Response keys: ['code', 'message', 'status']\n",
            "ğŸ“ API Message: **symbol** or **figi** parameter is missing or invalid. Please provide a valid symbol according to API documentation: https://twelvedata.com/docs#reference-data\n",
            "âŒ All symbol formats failed for 5min\n",
            "ğŸ”„ Trying yfinance fallback for 5min...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:$BTC-USD: possibly delisted; no price data found  (period=1y) (Yahoo error = \"5m data not available for startTime=1718918556 and endTime=1750454556. The requested range must be within the last 60 days.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ² Generating synthetic 5min data...\n",
            "âœ… Synthetic 5min data generated: 100 records\n",
            "ğŸ”§ Creating multi-timeframe features...\n",
            "Processing 15min...\n",
            "Processing 1h...\n",
            "Processing 5min...\n",
            "âœ… Multi-timeframe features created: (200, 50)\n",
            "ğŸ”§ Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ 3 features: 49 â†’ 52\n",
            "\n",
            "ğŸ¯ Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢:\n",
            "ğŸ”® Ğ¡Ğ¸Ğ³Ğ½Ğ°Ğ»: BUY ğŸŸ¢\n",
            "ğŸ¯ Confidence: 49.6%\n",
            "ğŸ“ˆ Buy: 49.6%\n",
            "ğŸ“Š Hold: 42.9%\n",
            "ğŸ“‰ Sell: 7.5%\n",
            "âŒ ĞĞ˜Ğ—ĞšĞĞ¯ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.int64(1), np.float32(0.49592036))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}